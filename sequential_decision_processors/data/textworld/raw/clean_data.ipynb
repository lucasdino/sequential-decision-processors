{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "113e4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from typing import Optional, Dict, Any, List\n",
    "import utils as util\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd2099",
   "metadata": {},
   "source": [
    "### Update file to be in correct format for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fad3cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 60000, Errors: 0\n",
      "Output saved to: cleaned_data/cooking_multimove_60k.jsonl\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"cleaned_data\"\n",
    "INPUT_FILENAME = \"cooking_multimove_9_29_25_60000.jsonl\"\n",
    "OUTPUT_FILENAME = \"cooking_multimove_60k.jsonl\"\n",
    "# INPUT_FILENAME = \"cooking_singlemove_9_29_25_150000.jsonl\"\n",
    "# OUTPUT_FILENAME = \"cooking_singlemove_150k.jsonl\"\n",
    "\n",
    "def parse_entry(entry):\n",
    "    data = entry['data']\n",
    "    \n",
    "    # Check first element is \"Full Observation\"\n",
    "    if data[0][0] != \"Full Observation\":\n",
    "        raise ValueError(f\"First element must be 'Full Observation', got: {data[0][0]}\")\n",
    "    \n",
    "    full_observation = data[0][1]\n",
    "    \n",
    "    # Parse alternating User/Environment pairs\n",
    "    user_env_pairs = data[1:]  # Skip the Full Observation\n",
    "    \n",
    "    # Validate pattern: User, Environment, User, Environment, ...\n",
    "    for i, (role, content) in enumerate(user_env_pairs):\n",
    "        expected_role = \"User\" if i % 2 == 0 else \"Environment\"\n",
    "        if role != expected_role:\n",
    "            raise ValueError(f\"Expected {expected_role} at index {i+1}, got: {role}\")\n",
    "    \n",
    "    # Build chat format\n",
    "    chat = [\n",
    "        [\"system\", \"generic_sysprompt.txt\"],\n",
    "        [\"user\", f\"You are playing Cooking World, a text-based game that requires you to assemble a recipe and eat a meal. This follows typical text-based adventure game formats -- you should attempt to output the best action to play directly given the current state of the game. I will provide you with the output from the environment. The current state is the following:\\n{full_observation}\\nWhat is your first action?\"]\n",
    "    ]\n",
    "    \n",
    "    # Add alternating assistant/user pairs\n",
    "    for i in range(0, len(user_env_pairs), 2):\n",
    "        # Add user action as assistant response\n",
    "        chat.append([\"assistant\", user_env_pairs[i][1]])\n",
    "        \n",
    "        # Add environment response as user (if exists)\n",
    "        if i + 1 < len(user_env_pairs):\n",
    "            chat.append([\"user\", user_env_pairs[i + 1][1]])\n",
    "    \n",
    "    return {\"chat\": chat}\n",
    "\n",
    "# Process file\n",
    "input_path = os.path.join(DATA_FOLDER, INPUT_FILENAME)\n",
    "output_path = os.path.join(DATA_FOLDER, OUTPUT_FILENAME)\n",
    "\n",
    "processed = 0\n",
    "errors = 0\n",
    "\n",
    "with open(input_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                entry = json.loads(line)\n",
    "                chat_entry = parse_entry(entry)\n",
    "                outfile.write(json.dumps(chat_entry) + '\\n')\n",
    "                processed += 1\n",
    "            except Exception as e:\n",
    "                errors += 1\n",
    "                print(f\"Error processing entry {processed + errors}: {e}\")\n",
    "\n",
    "print(f\"Processed: {processed}, Errors: {errors}\")\n",
    "print(f\"Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6696d19",
   "metadata": {},
   "source": [
    "### Analyze distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6d62c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing cleaned_data/cooking_singlemove_9_29_25_150000.jsonl...\n",
      "\n",
      "Total entries processed: 150000\n",
      "Entries with 'data': 150000\n",
      "Entries with 'move_idx': 150000\n",
      "\n",
      "=== Data Length Distribution ===\n",
      "Length -> Count (Percentage)\n",
      "     2 -> 150000 (100.0%)\n",
      "\n",
      "Data length stats:\n",
      "  Min: 2\n",
      "  Max: 2\n",
      "  Mean: 2.0\n",
      "\n",
      "=== Move Index Distribution ===\n",
      "Move Index -> Count (Percentage)\n",
      "         0 ->   5260 (  3.5%)\n",
      "         1 ->   5360 (  3.6%)\n",
      "         2 ->   5296 (  3.5%)\n",
      "         3 ->   5299 (  3.5%)\n",
      "         4 ->   5326 (  3.6%)\n",
      "         5 ->   5304 (  3.5%)\n",
      "         6 ->   5316 (  3.5%)\n",
      "         7 ->   5251 (  3.5%)\n",
      "         8 ->   5230 (  3.5%)\n",
      "         9 ->   5189 (  3.5%)\n",
      "        10 ->   5085 (  3.4%)\n",
      "        11 ->   4965 (  3.3%)\n",
      "        12 ->   4914 (  3.3%)\n",
      "        13 ->   4861 (  3.2%)\n",
      "        14 ->   4730 (  3.2%)\n",
      "        15 ->   4548 (  3.0%)\n",
      "        16 ->   4385 (  2.9%)\n",
      "        17 ->   4264 (  2.8%)\n",
      "        18 ->   4035 (  2.7%)\n",
      "        19 ->   3789 (  2.5%)\n",
      "        20 ->   3530 (  2.4%)\n",
      "        21 ->   3398 (  2.3%)\n",
      "        22 ->   3215 (  2.1%)\n",
      "        23 ->   3100 (  2.1%)\n",
      "        24 ->   2904 (  1.9%)\n",
      "        25 ->   2761 (  1.8%)\n",
      "        26 ->   2605 (  1.7%)\n",
      "        27 ->   2435 (  1.6%)\n",
      "        28 ->   2251 (  1.5%)\n",
      "        29 ->   2102 (  1.4%)\n",
      "        30 ->   1993 (  1.3%)\n",
      "        31 ->   1879 (  1.3%)\n",
      "        32 ->   1781 (  1.2%)\n",
      "        33 ->   1620 (  1.1%)\n",
      "        34 ->   1481 (  1.0%)\n",
      "        35 ->   1333 (  0.9%)\n",
      "        36 ->   1249 (  0.8%)\n",
      "        37 ->   1174 (  0.8%)\n",
      "        38 ->   1050 (  0.7%)\n",
      "        39 ->   1001 (  0.7%)\n",
      "        40 ->    880 (  0.6%)\n",
      "        41 ->    791 (  0.5%)\n",
      "        42 ->    722 (  0.5%)\n",
      "        43 ->    600 (  0.4%)\n",
      "        44 ->    522 (  0.3%)\n",
      "        45 ->    492 (  0.3%)\n",
      "        46 ->    431 (  0.3%)\n",
      "        47 ->    394 (  0.3%)\n",
      "        48 ->    370 (  0.2%)\n",
      "        49 ->    324 (  0.2%)\n",
      "        50 ->    291 (  0.2%)\n",
      "        51 ->    289 (  0.2%)\n",
      "        52 ->    262 (  0.2%)\n",
      "        53 ->    232 (  0.2%)\n",
      "        54 ->    223 (  0.1%)\n",
      "        55 ->    202 (  0.1%)\n",
      "        56 ->    170 (  0.1%)\n",
      "        57 ->    163 (  0.1%)\n",
      "        58 ->    178 (  0.1%)\n",
      "        59 ->    164 (  0.1%)\n",
      "        60 ->    125 (  0.1%)\n",
      "        61 ->    115 (  0.1%)\n",
      "        62 ->     94 (  0.1%)\n",
      "        63 ->     91 (  0.1%)\n",
      "        64 ->     68 (  0.0%)\n",
      "        65 ->     67 (  0.0%)\n",
      "        66 ->     55 (  0.0%)\n",
      "        67 ->     55 (  0.0%)\n",
      "        68 ->     51 (  0.0%)\n",
      "        69 ->     43 (  0.0%)\n",
      "        70 ->     43 (  0.0%)\n",
      "        71 ->     29 (  0.0%)\n",
      "        72 ->     26 (  0.0%)\n",
      "        73 ->     20 (  0.0%)\n",
      "        74 ->     21 (  0.0%)\n",
      "        75 ->     16 (  0.0%)\n",
      "        76 ->     14 (  0.0%)\n",
      "        77 ->     16 (  0.0%)\n",
      "        78 ->     12 (  0.0%)\n",
      "        79 ->      9 (  0.0%)\n",
      "        80 ->      8 (  0.0%)\n",
      "        81 ->      8 (  0.0%)\n",
      "        82 ->      4 (  0.0%)\n",
      "        83 ->      4 (  0.0%)\n",
      "        84 ->      9 (  0.0%)\n",
      "        85 ->      1 (  0.0%)\n",
      "        86 ->      1 (  0.0%)\n",
      "        87 ->      3 (  0.0%)\n",
      "        88 ->      2 (  0.0%)\n",
      "        89 ->      3 (  0.0%)\n",
      "        90 ->      2 (  0.0%)\n",
      "        91 ->      3 (  0.0%)\n",
      "        92 ->      2 (  0.0%)\n",
      "        93 ->      1 (  0.0%)\n",
      "        94 ->      2 (  0.0%)\n",
      "        95 ->      4 (  0.0%)\n",
      "        96 ->      2 (  0.0%)\n",
      "        97 ->      1 (  0.0%)\n",
      "        98 ->      1 (  0.0%)\n",
      "\n",
      "Move index stats:\n",
      "  Min: 0\n",
      "  Max: 98\n",
      "  Mean: 16.6\n",
      "\n",
      "Total entries processed: 150000\n",
      "Entries with 'data': 150000\n",
      "Entries with 'move_idx': 150000\n",
      "\n",
      "=== Data Length Distribution ===\n",
      "Length -> Count (Percentage)\n",
      "     2 -> 150000 (100.0%)\n",
      "\n",
      "Data length stats:\n",
      "  Min: 2\n",
      "  Max: 2\n",
      "  Mean: 2.0\n",
      "\n",
      "=== Move Index Distribution ===\n",
      "Move Index -> Count (Percentage)\n",
      "         0 ->   5260 (  3.5%)\n",
      "         1 ->   5360 (  3.6%)\n",
      "         2 ->   5296 (  3.5%)\n",
      "         3 ->   5299 (  3.5%)\n",
      "         4 ->   5326 (  3.6%)\n",
      "         5 ->   5304 (  3.5%)\n",
      "         6 ->   5316 (  3.5%)\n",
      "         7 ->   5251 (  3.5%)\n",
      "         8 ->   5230 (  3.5%)\n",
      "         9 ->   5189 (  3.5%)\n",
      "        10 ->   5085 (  3.4%)\n",
      "        11 ->   4965 (  3.3%)\n",
      "        12 ->   4914 (  3.3%)\n",
      "        13 ->   4861 (  3.2%)\n",
      "        14 ->   4730 (  3.2%)\n",
      "        15 ->   4548 (  3.0%)\n",
      "        16 ->   4385 (  2.9%)\n",
      "        17 ->   4264 (  2.8%)\n",
      "        18 ->   4035 (  2.7%)\n",
      "        19 ->   3789 (  2.5%)\n",
      "        20 ->   3530 (  2.4%)\n",
      "        21 ->   3398 (  2.3%)\n",
      "        22 ->   3215 (  2.1%)\n",
      "        23 ->   3100 (  2.1%)\n",
      "        24 ->   2904 (  1.9%)\n",
      "        25 ->   2761 (  1.8%)\n",
      "        26 ->   2605 (  1.7%)\n",
      "        27 ->   2435 (  1.6%)\n",
      "        28 ->   2251 (  1.5%)\n",
      "        29 ->   2102 (  1.4%)\n",
      "        30 ->   1993 (  1.3%)\n",
      "        31 ->   1879 (  1.3%)\n",
      "        32 ->   1781 (  1.2%)\n",
      "        33 ->   1620 (  1.1%)\n",
      "        34 ->   1481 (  1.0%)\n",
      "        35 ->   1333 (  0.9%)\n",
      "        36 ->   1249 (  0.8%)\n",
      "        37 ->   1174 (  0.8%)\n",
      "        38 ->   1050 (  0.7%)\n",
      "        39 ->   1001 (  0.7%)\n",
      "        40 ->    880 (  0.6%)\n",
      "        41 ->    791 (  0.5%)\n",
      "        42 ->    722 (  0.5%)\n",
      "        43 ->    600 (  0.4%)\n",
      "        44 ->    522 (  0.3%)\n",
      "        45 ->    492 (  0.3%)\n",
      "        46 ->    431 (  0.3%)\n",
      "        47 ->    394 (  0.3%)\n",
      "        48 ->    370 (  0.2%)\n",
      "        49 ->    324 (  0.2%)\n",
      "        50 ->    291 (  0.2%)\n",
      "        51 ->    289 (  0.2%)\n",
      "        52 ->    262 (  0.2%)\n",
      "        53 ->    232 (  0.2%)\n",
      "        54 ->    223 (  0.1%)\n",
      "        55 ->    202 (  0.1%)\n",
      "        56 ->    170 (  0.1%)\n",
      "        57 ->    163 (  0.1%)\n",
      "        58 ->    178 (  0.1%)\n",
      "        59 ->    164 (  0.1%)\n",
      "        60 ->    125 (  0.1%)\n",
      "        61 ->    115 (  0.1%)\n",
      "        62 ->     94 (  0.1%)\n",
      "        63 ->     91 (  0.1%)\n",
      "        64 ->     68 (  0.0%)\n",
      "        65 ->     67 (  0.0%)\n",
      "        66 ->     55 (  0.0%)\n",
      "        67 ->     55 (  0.0%)\n",
      "        68 ->     51 (  0.0%)\n",
      "        69 ->     43 (  0.0%)\n",
      "        70 ->     43 (  0.0%)\n",
      "        71 ->     29 (  0.0%)\n",
      "        72 ->     26 (  0.0%)\n",
      "        73 ->     20 (  0.0%)\n",
      "        74 ->     21 (  0.0%)\n",
      "        75 ->     16 (  0.0%)\n",
      "        76 ->     14 (  0.0%)\n",
      "        77 ->     16 (  0.0%)\n",
      "        78 ->     12 (  0.0%)\n",
      "        79 ->      9 (  0.0%)\n",
      "        80 ->      8 (  0.0%)\n",
      "        81 ->      8 (  0.0%)\n",
      "        82 ->      4 (  0.0%)\n",
      "        83 ->      4 (  0.0%)\n",
      "        84 ->      9 (  0.0%)\n",
      "        85 ->      1 (  0.0%)\n",
      "        86 ->      1 (  0.0%)\n",
      "        87 ->      3 (  0.0%)\n",
      "        88 ->      2 (  0.0%)\n",
      "        89 ->      3 (  0.0%)\n",
      "        90 ->      2 (  0.0%)\n",
      "        91 ->      3 (  0.0%)\n",
      "        92 ->      2 (  0.0%)\n",
      "        93 ->      1 (  0.0%)\n",
      "        94 ->      2 (  0.0%)\n",
      "        95 ->      4 (  0.0%)\n",
      "        96 ->      2 (  0.0%)\n",
      "        97 ->      1 (  0.0%)\n",
      "        98 ->      1 (  0.0%)\n",
      "\n",
      "Move index stats:\n",
      "  Min: 0\n",
      "  Max: 98\n",
      "  Mean: 16.6\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = \"cleaned_data\"\n",
    "FILENAME = \"cooking_multimove_9_29_25_60000.jsonl\"\n",
    "# FILENAME = \"cooking_singlemove_9_29_25_150000.jsonl\"\n",
    "\n",
    "# Load our data in -- for each line we have something like the following:\n",
    "#{\"data\": [[\"Full Observation\", \"Move 1 | Score = 0/15\\nActionable Verbs: ['chop', 'close', 'cook', 'dice', 'drink', 'drop', 'eat', 'examine', 'go', 'insert', 'inventory', 'lock', 'look', 'open', 'prepare', 'put', 'slice', 'take', 'unlock']\\nObjective: You are hungry! Let's cook a delicious meal. Check the cookbook in the kitchen for the recipe. Once done, enjoy your meal!\\nLocation: Pantry\\nInventory: You are carrying: a yellow bell pepper, a banana, an orange bell pepper, a raw purple potato, a red hot pepper and a raw yellow potato.\\nCurrent Observation: You've entered a pantry.\\n\\nOh, great. Here's a shelf. The shelf is wooden. But oh no! there's nothing on this piece of garbage. Aw, and here you were, all excited for there to be things on it!\\n\\nThere is a closed plain door leading east.\\nNecessary Context: Recipe #1\\n---------\\nGather all following ingredients and follow the directions to prepare this tasty meal.\\n\\nIngredients:\\nblock of cheese\\n  red potato\\n  banana\\n  yellow bell pepper\\n  red onion\\n\\nDirections:\\nchop the block of cheese\\n  roast the block of cheese\\n  slice the red potato\\n  roast the red potato\\n  dice the banana\\n  roast the banana\\n  dice the yellow bell pepper\\n  roast the yellow bell pepper\\n  dice the red onion\\n  roast the red onion\\n  prepare meal\\n\"], [\"User\", \"Open plain door\"], [\"Environment\", \"You open plain door. [Move 2 | Score = 0/15]\"], [\"User\", \"Go east\"], [\"Environment\", \"-= Kitchen =-\\nYou've just sauntered into a kitchen. Okay, just remember what you're here to do, and everything will go great.\\n\\nYou can make out a closed fridge. You can see a closed oven. You can see a table. The table is massive. On the table you see a knife. As if things weren't amazing enough already, you can even see a counter. You see a raw red potato, a red apple and a cookbook on the counter. You see a gleam over in a corner, where you can see a stove. But the thing is empty.\\n\\nThere is an open plain door leading west. There is an exit to the north. There is an exit to the south. [Move 3 | Score = 0/15]\"], [\"User\", \"Examine cookbook\"]], \"info\": {\"move_idx\": 1, \"generation_args\": {\"filename\": \"custom_cooking\", \"folder\": \"tw_games\", \"fmt\": \"z8\", \"split\": \"train\", \"seed\": 1482702892, \"recipe_seed\": 606939935, \"recipe\": 5, \"take\": 1, \"go\": 6, \"open_\": true, \"cook\": true, \"cut\": true, \"drop\": false}}}\n",
    "\n",
    "# First let's get a distribution of length of our 'data' array\n",
    "# Let's also go through and get a distribution of move_idx\n",
    "\n",
    "# Load the data and collect statistics\n",
    "data_lengths = []\n",
    "move_indices = []\n",
    "total_entries = 0\n",
    "\n",
    "filepath = os.path.join(DATA_FOLDER, FILENAME)\n",
    "print(f\"Analyzing {filepath}...\")\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            entry = json.loads(line)\n",
    "            total_entries += 1\n",
    "            \n",
    "            # Get length of 'data' array\n",
    "            if 'data' in entry:\n",
    "                data_lengths.append(len(entry['data']))\n",
    "            \n",
    "            # Get move_idx from info\n",
    "            if 'info' in entry and 'move_idx' in entry['info']:\n",
    "                move_indices.append(entry['info']['move_idx'])\n",
    "\n",
    "# Calculate distributions\n",
    "data_length_dist = Counter(data_lengths)\n",
    "move_idx_dist = Counter(move_indices)\n",
    "\n",
    "print(f\"\\nTotal entries processed: {total_entries}\")\n",
    "print(f\"Entries with 'data': {len(data_lengths)}\")\n",
    "print(f\"Entries with 'move_idx': {len(move_indices)}\")\n",
    "\n",
    "print(\"\\n=== Data Length Distribution ===\")\n",
    "print(\"Length -> Count (Percentage)\")\n",
    "for length in sorted(data_length_dist.keys()):\n",
    "    count = data_length_dist[length]\n",
    "    percentage = (count / len(data_lengths)) * 100 if data_lengths else 0\n",
    "    print(f\"{length:6d} -> {count:6d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nData length stats:\")\n",
    "if data_lengths:\n",
    "    print(f\"  Min: {min(data_lengths)}\")\n",
    "    print(f\"  Max: {max(data_lengths)}\")\n",
    "    print(f\"  Mean: {sum(data_lengths) / len(data_lengths):.1f}\")\n",
    "\n",
    "print(\"\\n=== Move Index Distribution ===\")\n",
    "print(\"Move Index -> Count (Percentage)\")\n",
    "for move_idx in sorted(move_idx_dist.keys()):\n",
    "    count = move_idx_dist[move_idx]\n",
    "    percentage = (count / len(move_indices)) * 100 if move_indices else 0\n",
    "    print(f\"{move_idx:10d} -> {count:6d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nMove index stats:\")\n",
    "if move_indices:\n",
    "    print(f\"  Min: {min(move_indices)}\")\n",
    "    print(f\"  Max: {max(move_indices)}\")\n",
    "    print(f\"  Mean: {sum(move_indices) / len(move_indices):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b8a81",
   "metadata": {},
   "source": [
    "### Aggregate multiple files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf7a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"generated_data/archive\"\n",
    "FILES = [\n",
    "    \"cooking_singlemove_20250928_231710_25000.jsonl\",\n",
    "    \"cooking_singlemove_20250929_013858_25000.jsonl\",\n",
    "    \"cooking_singlemove_20250929_025654_25000.jsonl\",\n",
    "    \"cooking_singlemove_20250929_041338_25000.jsonl\",\n",
    "    \"cooking_singlemove_20250929_053040_25000.jsonl\",\n",
    "    \"cooking_singlemove_20250929_064903_25000.jsonl\",\n",
    "]\n",
    "# FILES = [\n",
    "#     \"cooking_multimove_20250928_231710_10000.jsonl\",\n",
    "#     \"cooking_multimove_20250929_013858_10000.jsonl\",\n",
    "#     \"cooking_multimove_20250929_025654_10000.jsonl\",\n",
    "#     \"cooking_multimove_20250929_041338_10000.jsonl\",\n",
    "#     \"cooking_multimove_20250929_053040_10000.jsonl\",\n",
    "#     \"cooking_multimove_20250929_064903_10000.jsonl\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76ea183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cooking_singlemove_20250928_231710_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_013858_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_013858_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_025654_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_025654_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_041338_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_041338_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_053040_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_053040_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_064903_25000.jsonl...\n",
      "Processing cooking_singlemove_20250929_064903_25000.jsonl...\n",
      "Writing 150000 entries to generated_data/archive/cooking_singlemove_9_29_25_150000.jsonl...\n",
      "Writing 150000 entries to generated_data/archive/cooking_singlemove_9_29_25_150000.jsonl...\n",
      "Successfully created generated_data/archive/cooking_singlemove_9_29_25_150000.jsonl with 150000 entries\n",
      "Successfully created generated_data/archive/cooking_singlemove_9_29_25_150000.jsonl with 150000 entries\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"cooking_singlemove_9_29_25\"\n",
    "all_data = []\n",
    "\n",
    "# Read all files and collect data\n",
    "for filename in FILES:\n",
    "    filepath = os.path.join(DATA_FOLDER, filename)\n",
    "    print(f\"Processing {filename}...\")\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():  # Skip empty lines\n",
    "                data = json.loads(line)\n",
    "                all_data.append(data)\n",
    "\n",
    "# Create output filename with the total count\n",
    "total_count = len(all_data)\n",
    "output_filepath = os.path.join(DATA_FOLDER, f\"{output_filename}_{total_count}.jsonl\")\n",
    "\n",
    "# Write all data to the output file\n",
    "print(f\"Writing {total_count} entries to {output_filepath}...\")\n",
    "with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "    for data in all_data:\n",
    "        f.write(json.dumps(data) + '\\n')\n",
    "\n",
    "print(f\"Successfully created {output_filepath} with {total_count} entries\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
